{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Food Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pathlib\n",
    "import os\n",
    "import fnmatch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import IPython\n",
    "# make sure we use tensorflow 2.0\n",
    "import tensorflow as tf\n",
    "# print(tf.__version__)\n",
    "\n",
    "# import padding library\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# import our model, different layers and activation function \n",
    "from tensorflow.keras.layers import Dense, LSTM, Reshape, BatchNormalization, Input, Conv2D, MaxPool2D, Lambda, Bidirectional, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.activations import relu, sigmoid, softmax\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import CSVLogger, TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Conv2D,MaxPool2D,Activation,GlobalAveragePooling2D,BatchNormalization,Flatten,Dense,Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "Num GPUs Available:  1\n",
      "/device:GPU:0\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 6721308252498532172\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16787196714261914756\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 7245153349148330375\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15287485760\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 14382107032506434576\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.test.gpu_device_name())\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.backend as K\n",
    "\n",
    "# dtype='float16'\n",
    "# K.set_floatx(dtype)\n",
    "\n",
    "# # default is 1e-7 which is too small for float16.  Without adjusting the epsilon, we will get NaN predictions because of divide by zero problems\n",
    "# K.set_epsilon(1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in /opt/conda/lib/python3.7/site-packages (0.57.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from memory_profiler) (5.7.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q h5py\n",
    "!pip install memory_profiler\n",
    "%load_ext memory_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 522.14 MiB, increment: 0.07 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/FoodDetector/FoodClassification\n"
     ]
    }
   ],
   "source": [
    "root_path = os.getcwd()\n",
    "print(root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw result plot\n",
    "def plot_history(history, key='loss'):\n",
    "  plt.figure(figsize=(12,8))\n",
    "  val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                  '--', label=key.title() +' Val')\n",
    "  plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
    "              label=key.title() + ' Train')\n",
    "\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel(key.replace('_',' ').title())\n",
    "  plt.legend()\n",
    "\n",
    "  plt.xlim([0,max(history.epoch)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter/FoodDetector/FoodClassification/datasets/UECFOOD256crop_food_labels_paths.csv\n"
     ]
    }
   ],
   "source": [
    "labels_paths_path = os.path.join(root_path,'datasets/UECFOOD256crop_food_labels_paths.csv')\n",
    "print(labels_paths_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31395"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(labels_paths_path)\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31395 entries, 0 to 31394\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Label   31395 non-null  object\n",
      " 1   Path    31395 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 490.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rice</td>\n",
       "      <td>/home/jupyter/FoodDetector/datasets/UECFOOD256...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rice</td>\n",
       "      <td>/home/jupyter/FoodDetector/datasets/UECFOOD256...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rice</td>\n",
       "      <td>/home/jupyter/FoodDetector/datasets/UECFOOD256...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rice</td>\n",
       "      <td>/home/jupyter/FoodDetector/datasets/UECFOOD256...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rice</td>\n",
       "      <td>/home/jupyter/FoodDetector/datasets/UECFOOD256...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Label                                               Path\n",
       "0  rice  /home/jupyter/FoodDetector/datasets/UECFOOD256...\n",
       "1  rice  /home/jupyter/FoodDetector/datasets/UECFOOD256...\n",
       "2  rice  /home/jupyter/FoodDetector/datasets/UECFOOD256...\n",
       "3  rice  /home/jupyter/FoodDetector/datasets/UECFOOD256...\n",
       "4  rice  /home/jupyter/FoodDetector/datasets/UECFOOD256..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "miso soup                  728\n",
       "rice                       620\n",
       "ramen noodle               353\n",
       "green salad                342\n",
       "beef curry                 246\n",
       "                          ... \n",
       "green curry                100\n",
       "Crispy Noodles             100\n",
       "noodles with fish curry    100\n",
       "chop suey                  100\n",
       "Pork with lemon            100\n",
       "Name: Label, Length: 255, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['rice', 'eels on rice', 'pilaf', \"chicken-'n'-egg on rice\",\n",
       "       'pork cutlet on rice', 'beef curry', 'sushi', 'chicken rice',\n",
       "       'fried rice', 'tempura bowl', 'bibimbap', 'toast', 'croissant',\n",
       "       'roll bread', 'raisin bread', 'chip butty', 'hamburger', 'pizza',\n",
       "       'sandwiches', 'udon noodle', 'tempura udon', 'soba noodle',\n",
       "       'ramen noodle', 'beef noodle', 'tensin noodle', 'fried noodle',\n",
       "       'spaghetti', 'Japanese-style pancake', 'takoyaki', 'gratin',\n",
       "       'sauteed vegetables', 'croquette', 'grilled eggplant',\n",
       "       'sauteed spinach', 'vegetable tempura', 'miso soup', 'potage',\n",
       "       'sausage', 'oden', 'omelet', 'ganmodoki', 'jiaozi', 'stew',\n",
       "       'teriyaki grilled fish', 'fried fish', 'grilled salmon',\n",
       "       'salmon meuniere', 'sashimi', 'grilled pacific saury', 'sukiyaki',\n",
       "       'sweet and sour pork', 'lightly roasted fish',\n",
       "       'steamed egg hotchpotch', 'tempura', 'fried chicken',\n",
       "       'sirloin cutlet', 'nanbanzuke', 'boiled fish',\n",
       "       'seasoned beef with potatoes', 'hambarg steak', 'steak',\n",
       "       'dried fish', 'ginger pork saute', 'spicy chili-flavored tofu',\n",
       "       'yakitori', 'cabbage roll', 'egg sunny-side up', 'natto',\n",
       "       'cold tofu', 'egg roll', 'chilled noodle',\n",
       "       'stir-fried beef and peppers', 'simmered pork',\n",
       "       'boiled chicken and vegetables', 'sashimi bowl', 'sushi bowl',\n",
       "       'fish-shaped pancake with bean jam', 'shrimp with chill source',\n",
       "       'roast chicken', 'steamed meat dumpling', 'omelet with fried rice',\n",
       "       'cutlet curry', 'spaghetti meat sauce', 'fried shrimp',\n",
       "       'potato salad', 'green salad', 'macaroni salad',\n",
       "       'Japanese tofu and vegetable chowder', 'pork miso soup',\n",
       "       'chinese soup', 'beef bowl', 'kinpira-style sauteed burdock',\n",
       "       'rice ball', 'pizza toast', 'dipping noodles', 'hot dog',\n",
       "       'french fries', 'mixed rice', 'goya chanpuru', 'green curry',\n",
       "       'okinawa soba', 'mango pudding', 'almond jelly', 'jjigae',\n",
       "       'dak galbi', 'dry curry', 'kamameshi', 'rice vermicelli', 'paella',\n",
       "       'tanmen', 'kushikatu', 'yellow curry', 'pancake', 'champon',\n",
       "       'crape', 'tiramisu', 'waffle', 'rare cheese cake', 'shortcake',\n",
       "       'chop suey', 'twice cooked pork', 'mushroom risotto', 'samul',\n",
       "       'zoni', 'french toast', 'fine white noodles', 'minestrone',\n",
       "       'pot au feu', 'chicken nugget', 'namero', 'french bread',\n",
       "       'rice gruel', 'broiled eel bowl', 'clear soup', 'yudofu', 'mozuku',\n",
       "       'inarizushi', 'pork loin cutlet', 'pork fillet cutlet',\n",
       "       'chicken cutlet', 'ham cutlet', 'minced meat cutlet',\n",
       "       'thinly sliced raw horsemeat', 'bagel', 'scone', 'tortilla',\n",
       "       'tacos', 'nachos', 'meat loaf', 'scrambled egg', 'rice gratin',\n",
       "       'lasagna', 'Caesar salad', 'oatmeal',\n",
       "       'fried pork dumplings served in soup', 'oshiruko', 'muffin',\n",
       "       'popcorn', 'cream puff', 'doughnut', 'apple pie', 'parfait',\n",
       "       'fried pork in scoop', 'lamb kebabs',\n",
       "       'dish consisting of stir-fried potato, eggplant and green pepper',\n",
       "       'roast duck', 'hot pot', 'pork belly', 'xiao long bao',\n",
       "       'moon cake', 'custard tart', 'beef noodle soup', 'pork cutlet',\n",
       "       'minced pork rice', 'fish ball soup', 'oyster omelette',\n",
       "       'glutinous oil rice', 'trunip pudding', 'stinky tofu',\n",
       "       'lemon fig jelly', 'khao soi', 'Sour prawn soup',\n",
       "       'Thai papaya salad',\n",
       "       'boned, sliced Hainan-style chicken with marinated rice',\n",
       "       'hot and sour, fish and vegetable ragout',\n",
       "       'stir-fried mixed vegetables', 'beef in oyster sauce',\n",
       "       'pork satay', 'spicy chicken salad', 'noodles with fish curry',\n",
       "       'Pork Sticky Noodles', 'Pork with lemon', 'stewed pork leg',\n",
       "       'charcoal-boiled pork neck', 'fried mussel pancakes',\n",
       "       'Deep Fried Chicken Wing', 'Barbecued red pork in sauce with rice',\n",
       "       'Rice with roast duck', 'Rice crispy pork', 'Wonton soup',\n",
       "       'Chicken Rice Curry With Coconut', 'Crispy Noodles',\n",
       "       'Egg Noodle In Chicken Yellow Curry', 'coconut milk soup', 'pho',\n",
       "       'Hue beef rice vermicelli soup', 'Vermicelli noodles with snails',\n",
       "       'Fried spring rolls', 'Steamed rice roll', 'Shrimp patties',\n",
       "       'ball shaped bun with pork',\n",
       "       'Coconut milk-flavored crepes with shrimp and beef',\n",
       "       'Small steamed savory rice pancake', 'Glutinous Rice Balls',\n",
       "       'loco moco', 'haupia', 'malasada', 'laulau', 'spam musubi',\n",
       "       'oxtail soup', 'adobo', 'lumpia', 'brownie', 'churro', 'jambalaya',\n",
       "       'nasi goreng', 'ayam goreng', 'ayam bakar', 'bubur ayam', 'gulai',\n",
       "       'laksa', 'mie ayam', 'mie goreng', 'nasi campur', 'nasi padang',\n",
       "       'nasi uduk', 'babi guling', 'kaya toast', 'bak kut teh',\n",
       "       'curry puff', 'chow mein', 'zha jiang mian', 'kung pao chicken',\n",
       "       'crullers', 'eggplant with garlic sauce', 'three cup chicken',\n",
       "       'bean curd family style', 'salt & pepper fried shrimp with shell',\n",
       "       'baked salmon', 'braised pork meat ball with napa cabbage',\n",
       "       'winter melon soup', 'steamed spareribs', 'chinese pumpkin pie',\n",
       "       'eight treasure rice', 'hot & sour soup'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_index = dict((name,index) for index,name in enumerate(df['Label'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Category'] = df['Label'].apply(lambda x: label_to_index[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Path</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23437</th>\n",
       "      <td>Thai papaya salad</td>\n",
       "      <td>/home/jupyter/FoodDetector/datasets/UECFOOD256...</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21887</th>\n",
       "      <td>xiao long bao</td>\n",
       "      <td>/home/jupyter/FoodDetector/datasets/UECFOOD256...</td>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31174</th>\n",
       "      <td>eight treasure rice</td>\n",
       "      <td>/home/jupyter/FoodDetector/datasets/UECFOOD256...</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2340</th>\n",
       "      <td>croissant</td>\n",
       "      <td>/home/jupyter/FoodDetector/datasets/UECFOOD256...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30481</th>\n",
       "      <td>salt &amp; pepper fried shrimp with shell</td>\n",
       "      <td>/home/jupyter/FoodDetector/datasets/UECFOOD256...</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Label  \\\n",
       "23437                      Thai papaya salad   \n",
       "21887                          xiao long bao   \n",
       "31174                    eight treasure rice   \n",
       "2340                               croissant   \n",
       "30481  salt & pepper fried shrimp with shell   \n",
       "\n",
       "                                                    Path  Category  \n",
       "23437  /home/jupyter/FoodDetector/datasets/UECFOOD256...       182  \n",
       "21887  /home/jupyter/FoodDetector/datasets/UECFOOD256...       168  \n",
       "31174  /home/jupyter/FoodDetector/datasets/UECFOOD256...       253  \n",
       "2340   /home/jupyter/FoodDetector/datasets/UECFOOD256...        12  \n",
       "30481  /home/jupyter/FoodDetector/datasets/UECFOOD256...       247  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "miso soup                  728\n",
       "rice                       620\n",
       "ramen noodle               353\n",
       "green salad                342\n",
       "beef curry                 246\n",
       "                          ... \n",
       "green curry                100\n",
       "Crispy Noodles             100\n",
       "noodles with fish curry    100\n",
       "chop suey                  100\n",
       "Pork with lemon            100\n",
       "Name: Label, Length: 255, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# rest, samples = train_test_split(df, test_size=0.03, \n",
    "#                                random_state=150, \n",
    "#                                stratify=df['Label'])\n",
    "train, test = train_test_split(df, test_size=0.2, \n",
    "                               random_state=150)\n",
    "# train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 555.01 MiB, increment: 0.02 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train, test and validation set\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "TRAIN_SIZE = train.shape[0]\n",
    "TEST_SIZE = test.shape[0]\n",
    "NUM_CLASSES = df['Label'].nunique()\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "MOBILE_NET_IMG_SIZE = 192\n",
    "XCEPTION_IMG_SIZE = 299\n",
    "EfficientNetB7_IMAGE_SIZE = 256\n",
    "IMAGE_SIZE = EfficientNetB7_IMAGE_SIZE\n",
    "\n",
    "\n",
    "STEPS_PER_TRAIN_EPOCH = tf.math.ceil(train.shape[0]/BATCH_SIZE)\n",
    "STEPS_PER_TEST_EPOCH = tf.math.ceil(test.shape[0]/BATCH_SIZE)\n",
    "\n",
    "\n",
    "# train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "# test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "    image /= 255.0\n",
    "\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)    \n",
    "    image = (image*2) - 1  # normalize to [-1,1] range\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def augmentation(image, label):\n",
    "#     image = tf.image.random_contrast(image, lower=0.0, upper=1.0,seed=115)    \n",
    "    image = tf.image.random_brightness(image, .5)\n",
    "    image = tf.image.random_contrast(image, lower=0.0, upper=1.0)\n",
    "    \n",
    "    image = tf.image.random_flip_left_right(image,seed=115)\n",
    "    image = tf.image.random_flip_up_down(image,seed=115)\n",
    "    return image, label\n",
    "\n",
    "def load_and_preprocess_from_path_and_label(path,label):\n",
    "    return preprocess(path), label\n",
    "\n",
    "def prepare_for_training(ds, cache=False,shuffle_buffer_size=100,augment=False):\n",
    "    if cache:\n",
    "        if isinstance(cache,str):\n",
    "            ds = ds.cache(cache)\n",
    "        else: \n",
    "            ds = ds.cache()\n",
    "    if shuffle_buffer_size > 0:\n",
    "        ds = ds.shuffle(buffer_size=shuffle_buffer_size)    \n",
    "    # repeat forever\n",
    "    ds = ds.repeat()\n",
    "    if augment:\n",
    "        ds.map(augmentation, num_parallel_calls=AUTOTUNE)\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    \n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train['Path'],train['Category']))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test['Path'],test['Category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: b'/home/jupyter/FoodDetector/datasets/UECFOOD256crop/UECFOOD256/218/211024.jpg', Target: 216\n",
      "Features: b'/home/jupyter/FoodDetector/datasets/UECFOOD256crop/UECFOOD256/116/11312.jpg', Target: 114\n",
      "Features: b'/home/jupyter/FoodDetector/datasets/UECFOOD256crop/UECFOOD256/87/97.jpg', Target: 85\n",
      "Features: b'/home/jupyter/FoodDetector/datasets/UECFOOD256crop/UECFOOD256/164/47899.jpg', Target: 162\n",
      "Features: b'/home/jupyter/FoodDetector/datasets/UECFOOD256crop/UECFOOD256/49/4976.jpg', Target: 48\n"
     ]
    }
   ],
   "source": [
    "for feat, targ in train_dataset.take(5):\n",
    "  print ('Features: {}, Target: {}'.format(feat, targ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1718.63 MiB, increment: 0.01 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.map(lambda path,label: load_and_preprocess_from_path_and_label(path,label),\n",
    "                       num_parallel_calls=AUTOTUNE)\n",
    "test_dataset = test_dataset.map(lambda path,label: load_and_preprocess_from_path_and_label(path,label),                      \n",
    "                       num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = prepare_for_training(train_dataset,shuffle_buffer_size=512,augment=True,cache=True)\n",
    "test_dataset = prepare_for_training(test_dataset,shuffle_buffer_size=512,cache=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: ((None, 256, 256, 3), (None,)), types: (tf.float32, tf.int64)>\n",
      "<PrefetchDataset shapes: ((None, 256, 256, 3), (None,)), types: (tf.float32, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1721.67 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: efficientnet in /opt/conda/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: scikit-image in /opt/conda/lib/python3.7/site-packages (from efficientnet) (0.17.1)\n",
      "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /opt/conda/lib/python3.7/site-packages (from efficientnet) (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (1.18.4)\n",
      "Requirement already satisfied: scipy>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (1.4.1)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (3.2.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (2.4)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (7.0.0)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (2.8.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (2020.5.7)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (1.1.1)\n",
      "Requirement already satisfied: pooch>=0.5.2 in /opt/conda/lib/python3.7/site-packages (from scikit-image->efficientnet) (1.1.0)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (2.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.2.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.4.7)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx>=2.0->scikit-image->efficientnet) (4.4.2)\n",
      "Requirement already satisfied: imagecodecs>=2020.2.18 in /opt/conda/lib/python3.7/site-packages (from tifffile>=2019.7.26->scikit-image->efficientnet) (2020.5.30)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from pooch>=0.5.2->scikit-image->efficientnet) (2.23.0)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.7/site-packages (from pooch>=0.5.2->scikit-image->efficientnet) (1.4.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from pooch>=0.5.2->scikit-image->efficientnet) (20.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from h5py->keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.15.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->pooch>=0.5.2->scikit-image->efficientnet) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->pooch>=0.5.2->scikit-image->efficientnet) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->pooch>=0.5.2->scikit-image->efficientnet) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->pooch>=0.5.2->scikit-image->efficientnet) (2.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet\n",
    "import efficientnet.tfkeras as efn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "class FocalLoss(keras.losses.Loss):\n",
    "    def __init__(self, gamma=2., alpha=4.,\n",
    "                 reduction=keras.losses.Reduction.AUTO, name='focal_loss'):\n",
    "        \"\"\"Focal loss for multi-classification\n",
    "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "        Notice: y_pred is probability after softmax\n",
    "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "        Focal Loss for Dense Object Detection\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})\n",
    "            alpha {float} -- (default: {4.0})\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__(reduction=reduction,\n",
    "                                        name=name)\n",
    "        self.gamma = float(gamma)\n",
    "        self.alpha = float(alpha)\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "\n",
    "        model_out = tf.add(y_pred, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.math.log(model_out))\n",
    "        weight = tf.multiply(y_true, tf.pow(\n",
    "            tf.subtract(1., model_out), self.gamma))\n",
    "        fl = tf.multiply(self.alpha, tf.multiply(weight, ce))\n",
    "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "        return tf.reduce_mean(reduced_fl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(load_from_path=None):\n",
    "    if load_from_path == None:\n",
    "        base_model =  efn.EfficientNetB7(weights='imagenet', \n",
    "                                         include_top=False, \n",
    "                                         input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
    "        base_model.trainable= True\n",
    "        \n",
    "        print(\"Number of layers in the base model:\", len(base_model.layers))\n",
    "        fine_tune_at = int(0.5 * len(base_model.layers))\n",
    "\n",
    "        for layer in base_model.layers[:fine_tune_at]:\n",
    "            layer.trainable = False\n",
    "        x = base_model.output\n",
    "        x1 = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "        x2 = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        x = tf.keras.layers.Concatenate()([x1, x2])\n",
    "        x = tf.keras.layers.BatchNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "        y = tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "        model = tf.keras.Model(inputs=base_model.input, outputs=y)\n",
    "        model.compile(loss='sparse_categorical_crossentropy',\n",
    "                      optimizer='nadam',\n",
    "                      metrics=[\"accuracy\"])\n",
    "    else:\n",
    "        model = tf.keras.models.load_model(load_from_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers in the base model: 806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7ff96e62a610>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_path = \"training/cp.ckpt\"\n",
    "model = create_model()\n",
    "\n",
    "model.load_weights(checkpoint_path)\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
    "                                                          model.optimizer.lr.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2142.43 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "785/785 [==============================] - ETA: 0s - loss: 1.1183 - accuracy: 0.6984\n",
      "Epoch 00001: val_loss improved from inf to 1.32716, saving model to training/cp.ckpt\n",
      "\n",
      "Learning rate for epoch 1 is 0.0010000000474974513\n",
      "785/785 [==============================] - 290s 370ms/step - loss: 1.1183 - accuracy: 0.6984 - val_loss: 1.3272 - val_accuracy: 0.6448 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "785/785 [==============================] - ETA: 0s - loss: 1.0494 - accuracy: 0.7188\n",
      "Epoch 00002: val_loss did not improve from 1.32716\n",
      "\n",
      "Learning rate for epoch 2 is 0.00020000000949949026\n",
      "785/785 [==============================] - 273s 348ms/step - loss: 1.0494 - accuracy: 0.7188 - val_loss: 1.4716 - val_accuracy: 0.6190 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "785/785 [==============================] - ETA: 0s - loss: 0.6991 - accuracy: 0.8088\n",
      "Epoch 00003: val_loss improved from 1.32716 to 1.16738, saving model to training/cp.ckpt\n",
      "\n",
      "Learning rate for epoch 3 is 0.00020000000949949026\n",
      "785/785 [==============================] - 280s 357ms/step - loss: 0.6991 - accuracy: 0.8088 - val_loss: 1.1674 - val_accuracy: 0.7076 - lr: 2.0000e-04\n",
      "Epoch 4/50\n",
      "785/785 [==============================] - ETA: 0s - loss: 0.5680 - accuracy: 0.8477\n",
      "Epoch 00004: val_loss did not improve from 1.16738\n",
      "\n",
      "Learning rate for epoch 4 is 4.0000002627493814e-05\n",
      "785/785 [==============================] - 274s 349ms/step - loss: 0.5680 - accuracy: 0.8477 - val_loss: 1.2095 - val_accuracy: 0.7045 - lr: 2.0000e-04\n",
      "Epoch 5/50\n",
      "785/785 [==============================] - ETA: 0s - loss: 0.4869 - accuracy: 0.8710\n",
      "Epoch 00005: val_loss did not improve from 1.16738\n",
      "\n",
      "Learning rate for epoch 5 is 8.000000889296643e-06\n",
      "785/785 [==============================] - 274s 349ms/step - loss: 0.4869 - accuracy: 0.8710 - val_loss: 1.1905 - val_accuracy: 0.7173 - lr: 4.0000e-05\n",
      "Epoch 6/50\n",
      "785/785 [==============================] - ETA: 0s - loss: 0.4482 - accuracy: 0.8793\n",
      "Epoch 00006: val_loss did not improve from 1.16738\n",
      "\n",
      "Learning rate for epoch 6 is 1.6000001323845936e-06\n",
      "785/785 [==============================] - 274s 349ms/step - loss: 0.4482 - accuracy: 0.8793 - val_loss: 1.1944 - val_accuracy: 0.7167 - lr: 8.0000e-06\n",
      "Epoch 7/50\n",
      "785/785 [==============================] - ETA: 0s - loss: 0.4493 - accuracy: 0.8820\n",
      "Epoch 00007: val_loss did not improve from 1.16738\n",
      "\n",
      "Learning rate for epoch 7 is 3.200000264769187e-07\n",
      "785/785 [==============================] - 274s 349ms/step - loss: 0.4493 - accuracy: 0.8820 - val_loss: 1.1859 - val_accuracy: 0.7189 - lr: 1.6000e-06\n",
      "Epoch 8/50\n",
      "785/785 [==============================] - ETA: 0s - loss: 0.4418 - accuracy: 0.8834\n",
      "Epoch 00008: val_loss did not improve from 1.16738\n",
      "\n",
      "Learning rate for epoch 8 is 6.400000529538374e-08\n",
      "785/785 [==============================] - 274s 349ms/step - loss: 0.4418 - accuracy: 0.8834 - val_loss: 1.2108 - val_accuracy: 0.7180 - lr: 3.2000e-07\n",
      "Epoch 9/50\n",
      "666/785 [========================>.....] - ETA: 37s - loss: 0.4457 - accuracy: 0.8794"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "max_epochs = 50\n",
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1) # Create EarlyStopping Callback\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                 factor=0.2, # Create ReduceLROnPlateau Callback\n",
    "                                                 patience=1, \n",
    "                                                 mode='min',\n",
    "                                                 min_lr=1e-13)\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "# by default it saves the weights every epoch\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_best_only=True,\n",
    "                                                 verbose=1)\n",
    "print_lr = PrintLR()\n",
    "history = model.fit(train_dataset, epochs=max_epochs,     \n",
    "                    validation_steps=STEPS_PER_TEST_EPOCH,\n",
    "                    steps_per_epoch=STEPS_PER_TRAIN_EPOCH,\n",
    "                    validation_data=test_dataset,\n",
    "                    callbacks=[cp_callback,reduce_lr,earlystop,print_lr], # Add callback to training process\n",
    "                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved_model/EfficientNetB7_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
